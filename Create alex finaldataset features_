{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e31c6e88be8e48adb58c966a023c5a23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dcc80b18e0514db2aece3abbfc26db15","IPY_MODEL_37da157b2944488784c54d244504f47f","IPY_MODEL_0bc66297de1e4446879df217e20a1d9e"],"layout":"IPY_MODEL_765ce966b54b44b88c1b12dbee22136c"}},"dcc80b18e0514db2aece3abbfc26db15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b547004bb49842f1b63d81ede5c126da","placeholder":"​","style":"IPY_MODEL_c7763deaa7d9427cbf06d82056fc7bdd","value":"100%"}},"37da157b2944488784c54d244504f47f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a978290ae1f3407f8b5878cc4494393c","max":244408911,"min":0,"orientation":"horizontal","style":"IPY_MODEL_537c2277dac04801bcff46c3d12fa2ab","value":244408911}},"0bc66297de1e4446879df217e20a1d9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e90794152dd42368d6ea00a91f8254b","placeholder":"​","style":"IPY_MODEL_d571e68da5c043deb394ff199ee5092b","value":" 233M/233M [00:07&lt;00:00, 32.5MB/s]"}},"765ce966b54b44b88c1b12dbee22136c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b547004bb49842f1b63d81ede5c126da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7763deaa7d9427cbf06d82056fc7bdd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a978290ae1f3407f8b5878cc4494393c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"537c2277dac04801bcff46c3d12fa2ab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e90794152dd42368d6ea00a91f8254b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d571e68da5c043deb394ff199ee5092b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# import & mount"],"metadata":{"id":"qpShg0AyuQM-"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"KzUoXfjuuHEZ","executionInfo":{"status":"ok","timestamp":1679083138760,"user_tz":240,"elapsed":8108,"user":{"displayName":"Madeline Lee","userId":"09159153056267545522"}}},"outputs":[],"source":["import matplotlib\n","import matplotlib.pyplot as plt  # Most common visualization package that a lot of others are based on\n","\n","import numpy as np  # Common package for numerical methods\n","import pandas as pd  # Common package for data storeage/manipulation\n","import seaborn as sns  # Common package for statistical visualizations\n","import datetime\n","\n","from IPython.display import SVG\n","from graphviz import Source\n","\n","# Import useful packages from sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import export_graphviz\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_validate\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.cluster import KMeans\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_score, recall_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Torch and Time!\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms as transforms\n","\n","# Import portion of a package\n","import scipy.stats as stats\n","from sklearn.impute import SimpleImputer as Imputer  # Specific function from common machine learning package\n","\n","# import download to export .csv files\n","from google.colab import files\n","\n","from PIL import ImageFile"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqgtWKnUuWiL","executionInfo":{"status":"ok","timestamp":1679083157131,"user_tz":240,"elapsed":18389,"user":{"displayName":"Madeline Lee","userId":"09159153056267545522"}},"outputId":"d2dd1f66-7a64-4c83-bf43-1d28992d817a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import torchvision.models\n","alexnet = torchvision.models.alexnet(pretrained=True)\n","import os"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["e31c6e88be8e48adb58c966a023c5a23","dcc80b18e0514db2aece3abbfc26db15","37da157b2944488784c54d244504f47f","0bc66297de1e4446879df217e20a1d9e","765ce966b54b44b88c1b12dbee22136c","b547004bb49842f1b63d81ede5c126da","c7763deaa7d9427cbf06d82056fc7bdd","a978290ae1f3407f8b5878cc4494393c","537c2277dac04801bcff46c3d12fa2ab","7e90794152dd42368d6ea00a91f8254b","d571e68da5c043deb394ff199ee5092b"]},"id":"535zYpMrucHc","executionInfo":{"status":"ok","timestamp":1679083167140,"user_tz":240,"elapsed":10029,"user":{"displayName":"Madeline Lee","userId":"09159153056267545522"}},"outputId":"5456d6ef-53b8-46a0-d23d-aa5890b8dedd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/233M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e31c6e88be8e48adb58c966a023c5a23"}},"metadata":{}}]},{"cell_type":"markdown","source":["# features"],"metadata":{"id":"UZhXCJlawRlF"}},{"cell_type":"code","source":["new_path = '/content/gdrive/My Drive/APS360 Project/Alex_Net_Features_FinalDataset/'\n","\n","batch_size = 1\n","num_workers=1\n","\n","path = '/content/gdrive/My Drive/APS360 Project/FinalDataset'\n","\n","classes = []\n","for folder in os.scandir(path):\n"," classes.append(folder.name)\n","\n","classes.sort()\n","print(classes)\n","print(len(classes))"],"metadata":{"id":"3FitnjOdv-f9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679083167143,"user_tz":240,"elapsed":28,"user":{"displayName":"Madeline Lee","userId":"09159153056267545522"}},"outputId":"fb4b16d4-ef09-4156-a1bd-95b4d3b6f890"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['Almond', 'Apple', 'Asparagus', 'Avocado', 'Banana', 'Beef', 'Beetroot', 'Blackberry', 'Blueberry', 'Bread', 'Cabbage', 'Cantaloupe', 'Carrot', 'Cauliflower', 'Cheese', 'Chicken', 'Chillipepper', 'Corn', 'Cucumber', 'Dates', 'Egg', 'Eggplant', 'Fish', 'Galiamelon', 'Garlic', 'Ginger', 'Grapefruit', 'Grapes', 'Honeydew', 'Jalepeno', 'Kiwi', 'Leek', 'Lemon', 'Lettuce', 'Lime', 'Mango', 'Milk', 'Mushroom', 'Nectarine', 'Onion', 'Orange', 'Papaya', 'Passionfruit', 'Peach', 'Pear', 'Peas', 'Pepper', 'Pineapple', 'Pistachio', 'Plum', 'Pomegranate', 'Pork', 'Potato', 'Raddish', 'Roti', 'Satsumas', 'Soybeans', 'Spinach', 'Strawberry', 'Sweetpotato', 'Tofu', 'Tomato', 'Turnip', 'Watermelon', 'Zucchini']\n","65\n"]}]},{"cell_type":"code","source":["# define the transforms to be applied to the images\n","transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n","\n","pics = torchvision.datasets.ImageFolder(path, transform=transform)\n","\n","# split into training, validation, and testing sets (60,20,20)\n","train_size = int(0.60 * len(pics))\n","val_size = int(0.20 * len(pics))\n","test_size = len(pics) - train_size - val_size\n","train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(pics, [train_size, val_size, test_size])\n","\n","# create data loaders for each of the datasets\n","train_loader_alex = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n","val_loader_alex = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n","test_loader_alex = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n","\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","i = 0\n","for img, label in train_loader_alex:\n","    features = alexnet.features(img)\n","    features_tensor = torch.from_numpy(features.detach().numpy())\n","    folder_name = new_path + 'train/' + str(classes[label])\n","    if not os.path.isdir(folder_name):\n","      os.mkdir(folder_name)\n","    torch.save(features_tensor.squeeze(0), folder_name + '/' + str(i) + '.tensor')\n","    i += 1\n","print(\"Completed Train\")\n","i = 0\n","for img, label in val_loader_alex:\n","    features = alexnet.features(img)\n","    features_tensor = torch.from_numpy(features.detach().numpy())\n","    folder_name = new_path + 'val/' + str(classes[label])\n","    if not os.path.isdir(folder_name):\n","      os.mkdir(folder_name)\n","    torch.save(features_tensor.squeeze(0), folder_name + '/' + str(i) + '.tensor')\n","    i += 1\n","print(\"Completed Val\")\n","i = 0\n","for img, label in test_loader_alex:\n","    features = alexnet.features(img)\n","    features_tensor = torch.from_numpy(features.detach().numpy())\n","    folder_name = new_path + 'test/' + str(classes[label])\n","    if not os.path.isdir(folder_name):\n","      os.mkdir(folder_name)\n","    torch.save(features_tensor.squeeze(0), folder_name + '/' + str(i) + '.tensor')\n","    i += 1"],"metadata":{"id":"idA3weRr_Cms","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679086575551,"user_tz":240,"elapsed":3405593,"user":{"displayName":"Madeline Lee","userId":"09159153056267545522"}},"outputId":"4e505c9c-d770-496b-f0bd-011844cd7015"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/PIL/Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Completed Train\n","Completed Val\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/PIL/Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]}]}]}